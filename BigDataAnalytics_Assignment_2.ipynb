{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1vok5PDgKmc3WyTYIXDOzPJ2UrxS_BqLy",
      "authorship_tag": "ABX9TyP5VO01MjpjkWq+HTxwa3RF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akhil-kuma/akhil/blob/main/BigDataAnalytics_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA3_3INkOaG0",
        "outputId": "6794167e-84a6-47b1-e18f-6b78aa7b1b74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.88\n",
            "+-----------------+-----+----------+\n",
            "|         features|label|prediction|\n",
            "+-----------------+-----+----------+\n",
            "|[4.7,3.2,1.6,0.2]|  0.0|       0.0|\n",
            "|[5.0,3.6,1.4,0.2]|  0.0|       0.0|\n",
            "|[5.2,3.4,1.4,0.2]|  0.0|       0.0|\n",
            "|[5.2,3.5,1.5,0.2]|  0.0|       0.0|\n",
            "|[6.3,3.3,6.0,2.5]|  2.0|       2.0|\n",
            "|[6.4,3.2,4.5,1.5]|  1.0|       2.0|\n",
            "|[6.5,2.8,4.6,1.5]|  1.0|       1.0|\n",
            "|[7.1,3.0,5.9,2.1]|  2.0|       2.0|\n",
            "+-----------------+-----+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# 1. Start Spark Session\n",
        "spark = SparkSession.builder.appName(\"IrisClassification\").getOrCreate()\n",
        "\n",
        "# 2. Load the dataset (replace with your path if local)\n",
        "# Using a public URL for Iris CSV\n",
        "iris_url = \"/iris.csv\"\n",
        "df = spark.read.csv(iris_url, header=True, inferSchema=True)\n",
        "\n",
        "# 3. Encode label column\n",
        "indexer = StringIndexer(inputCol=\"species\", outputCol=\"label\")\n",
        "\n",
        "# 4. Combine feature columns into a single vector\n",
        "feature_cols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "\n",
        "# 5. Create a Logistic Regression model\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# 6. Build Pipeline\n",
        "pipeline = Pipeline(stages=[indexer, assembler, lr])\n",
        "\n",
        "# 7. Split data into training and test sets\n",
        "train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# 8. Train the model\n",
        "model = pipeline.fit(train_data)\n",
        "\n",
        "# 9. Make predictions\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# 10. Evaluate the model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# 11. Show predictions\n",
        "predictions.select(\"features\", \"label\", \"prediction\").show()\n",
        "\n",
        "# 12. Stop Spark\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Start Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"IrisClustering\").getOrCreate()\n",
        "\n",
        "# Step 2: Load Dataset (Assumes iris.csv is in same directory)\n",
        "df = spark.read.csv(\"/iris.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Step 3: Assemble Features\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "data = assembler.transform(df)\n",
        "data = data.select(\"features\")\n",
        "\n",
        "# Step 4: Apply KMeans Clustering\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "kmeans = KMeans(k=3, seed=42)  # 3 clusters for 3 species\n",
        "model = kmeans.fit(data)\n",
        "predictions = model.transform(data)\n",
        "\n",
        "# Step 5: Show Cluster Assignments\n",
        "predictions.show(10)\n",
        "\n",
        "# Step 6: Evaluate with Silhouette Score\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "evaluator = ClusteringEvaluator()\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(f\"Silhouette Score = {silhouette:.2f}\")\n",
        "\n",
        "# Step 7: Stop Spark\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BebDmoH1QwLM",
        "outputId": "09c4fccd-4247-43b3-c3cf-f859ecc81340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+----------+\n",
            "|         features|prediction|\n",
            "+-----------------+----------+\n",
            "|[5.1,3.5,1.4,0.2]|         1|\n",
            "|[4.9,3.0,1.4,0.2]|         1|\n",
            "|[4.7,3.2,1.3,0.2]|         1|\n",
            "|[4.6,3.1,1.5,0.2]|         1|\n",
            "|[5.0,3.6,1.4,0.2]|         1|\n",
            "|[7.0,3.2,4.7,1.4]|         0|\n",
            "|[6.4,3.2,4.5,1.5]|         0|\n",
            "|[6.9,3.1,4.9,1.5]|         0|\n",
            "|[5.5,2.3,4.0,1.3]|         2|\n",
            "|[6.5,2.8,4.6,1.5]|         0|\n",
            "+-----------------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Silhouette Score = 0.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Spark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Start Spark Session\n",
        "spark = SparkSession.builder.appName(\"SimpleRecommendationEngine\").getOrCreate()\n",
        "\n",
        "# Step 2: Create Ratings Dataset\n",
        "data = [\n",
        "    (1, 101, 5.0),\n",
        "    (1, 102, 3.0),\n",
        "    (1, 103, 2.5),\n",
        "    (2, 101, 2.0),\n",
        "    (2, 102, 2.5),\n",
        "    (2, 103, 5.0),\n",
        "    (2, 104, 4.0),\n",
        "    (3, 101, 2.5),\n",
        "    (3, 104, 4.5),\n",
        "    (4, 102, 4.0),\n",
        "    (4, 103, 3.0),\n",
        "    (4, 104, 5.0),\n",
        "]\n",
        "\n",
        "columns = [\"userId\", \"movieId\", \"rating\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Step 3: Show Sample Ratings\n",
        "df.show()\n",
        "\n",
        "# Step 4: Build ALS Model\n",
        "from pyspark.ml.recommendation import ALS\n",
        "\n",
        "als = ALS(\n",
        "    userCol=\"userId\",\n",
        "    itemCol=\"movieId\",\n",
        "    ratingCol=\"rating\",\n",
        "    nonnegative=True,\n",
        "    coldStartStrategy=\"drop\"\n",
        ")\n",
        "\n",
        "model = als.fit(df)\n",
        "\n",
        "# Step 5: Generate Top-2 Movie Recommendations for Each User\n",
        "userRecs = model.recommendForAllUsers(2)\n",
        "userRecs.show(truncate=False)\n",
        "\n",
        "# Step 6: Optional - Evaluate Model\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "predictions = model.transform(df)\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "\n",
        "# Step 7: Stop Spark\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibP5oAHfRjSg",
        "outputId": "d1b21b6c-d544-484d-9a65-b41cfc67a95b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+------+\n",
            "|userId|movieId|rating|\n",
            "+------+-------+------+\n",
            "|     1|    101|   5.0|\n",
            "|     1|    102|   3.0|\n",
            "|     1|    103|   2.5|\n",
            "|     2|    101|   2.0|\n",
            "|     2|    102|   2.5|\n",
            "|     2|    103|   5.0|\n",
            "|     2|    104|   4.0|\n",
            "|     3|    101|   2.5|\n",
            "|     3|    104|   4.5|\n",
            "|     4|    102|   4.0|\n",
            "|     4|    103|   3.0|\n",
            "|     4|    104|   5.0|\n",
            "+------+-------+------+\n",
            "\n",
            "+------+------------------------------------+\n",
            "|userId|recommendations                     |\n",
            "+------+------------------------------------+\n",
            "|1     |[{101, 4.730415}, {104, 3.305429}]  |\n",
            "|2     |[{103, 4.690695}, {104, 4.0107307}] |\n",
            "|3     |[{104, 4.3910356}, {103, 3.423058}] |\n",
            "|4     |[{104, 4.9122496}, {102, 3.8038363}]|\n",
            "+------+------------------------------------+\n",
            "\n",
            "Root Mean Squared Error (RMSE): 0.14\n"
          ]
        }
      ]
    }
  ]
}